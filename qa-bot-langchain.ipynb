{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA Bot with Langchain\n",
    "\n",
    "## Installing necessary libraries\n",
    "To ensure seamless execution of your scripts, and considering that certain functions within these scripts rely on external libraries, it's essential to install some prerequisite libraries before you begin. For this project, the key libraries you'll need are Gradio for creating user-friendly web interfaces and IBM watsonx AI for leveraging advanced LLM models from IBM watsonx's API.\n",
    "\n",
    "- **gradio** allows you to build interactive web applications quickly, making your AI models accessible to users with ease.\n",
    "- **ibm-watsonx-ai** for using LLMs from IBM's watsonx.ai.\n",
    "- **langchain**, **langchain-ibm**, **langchain-community** for using relevant features from Langchain.\n",
    "- **chromadb** for using the chroma database as a vector database.\n",
    "- **pypdf** is required for loading PDF documents.\n",
    "\n",
    "Here's how to install these packages (from your terminal):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing necessary pacakges in my_env\n",
    "pip install \\\n",
    "gradio==4.44.0 \\\n",
    "ibm-watsonx-ai==1.1.2  \\\n",
    "langchain==0.2.11 \\\n",
    "langchain-community==0.2.10 \\\n",
    "langchain-ibm==0.1.11 \\\n",
    "chromadb==0.4.24 \\\n",
    "pypdf==4.3.1 \\\n",
    "pydantic==2.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michael/repositories/cs_langchain/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from langchain_ibm import WatsonxLLM, WatsonxEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "import gradio as gr\n",
    "# You can use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the LLM\n",
    "You will now initialize the LLM by creating an instance of WatsonxLLM, a class in langchain_ibm. WatsonxLLM can use several underlying foundational models. In this particular example, you will use Mixtral 8x7B, although you could have used other models, such as Llama 3.1 405B. For a list of foundational models available at watsonx.ai, refer to this.\n",
    "\n",
    "To initialize the LLM, paste the following into qabot.py. Note that you are initializing the model with a temperature of 0.5, and allowing for the generation of a maximum of 256 tokens:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LLM\n",
    "def get_llm():\n",
    "    model_id = 'mistralai/mixtral-8x7b-instruct-v01'\n",
    "    parameters = {\n",
    "        GenParams.MAX_NEW_TOKENS: 256,\n",
    "        GenParams.TEMPERATURE: 0.5,\n",
    "    }\n",
    "    # Replace project_id with your project ID\n",
    "    project_id = \"*****\"\n",
    "    watsonx_llm = WatsonxLLM(\n",
    "        model_id=model_id,\n",
    "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        project_id=project_id,\n",
    "        params=parameters,\n",
    "    )\n",
    "    return watsonx_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the PDF document loader\n",
    "Next, you will define the PDF document loader. You will use the PyPDFLoader class from the langchain_community library to load PDF documents. The syntax is quite straightforward. First, you create the PDF loader as an instance of PyPDFLoader. Then, you load the document and return the loaded document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Document loader\n",
    "def document_loader(file):\n",
    "    loader = PyPDFLoader(file.name)\n",
    "    loaded_document = loader.load()\n",
    "    return loaded_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the text splitter\n",
    "The PDF document loader loads the document but does not split it into chunks when using the .load() method. Consequently, you must define a document splitter that will split the text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text splitter\n",
    "def text_splitter(data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the vector store\n",
    "Now that you have a way to load the PDF into text and split that text into chunks, you must define a way to embed and store those chunks in a vector database. Add the following code to define a function that embeds the chunks using a yet to be defined embedding model and stores the embeddings in a ChromaDB vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vector db\n",
    "def vector_database(chunks):\n",
    "    embedding_model = watsonx_embedding()\n",
    "    vectordb = Chroma.from_documents(chunks, embedding_model)\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the embedding model\n",
    "The above vector_database() function assumes the existence of a watsonx_embedding() function that loads an instance of an embedding model. This embedding model is needed to convert chunks of text into vector representations. The following code defines a watsonx_embedding() function that returns an instance of WatsonxEmbeddings, a class from langchain_ibm that generates embeddings. In this particular case, the embeddings are generated using IBM's Slate 125M English embeddings model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding model\n",
    "def watsonx_embedding():\n",
    "    embed_params = {\n",
    "        EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n",
    "        EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n",
    "    }\n",
    "    watsonx_embedding = WatsonxEmbeddings(\n",
    "        model_id=\"ibm/slate-125m-english-rtrvr\",\n",
    "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        # Replace project_id with your project ID\n",
    "        project_id=\"*****\",\n",
    "        params=embed_params,\n",
    "    )\n",
    "    return watsonx_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it does not matter to Python that watsonx_embedding() is defined after vector_database(). The order of these definitions could have been reversed, which would have resulted in no change in the underlying functionality of the bot.\n",
    "\n",
    "## Define the retriever\n",
    "Now that your vector store is defined, you must define a retriever that retrieves chunks of the document from it. In this particular case, you will define a vector store-based retriever that retrieves information using a simple similarity search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retriever\n",
    "def retriever(file):\n",
    "    splits = document_loader(file)\n",
    "    chunks = text_splitter(splits)\n",
    "    vectordb = vector_database(chunks)\n",
    "    retriever = vectordb.as_retriever()\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a question-answering chain\n",
    "Finally, it is time to define a question-answering chain! In this particular example, you will use RetrievalQA from langchain, a chain that performs natural-language question-answering over a data source using retrieval-augmented generation (RAG). Add the following code to define a question-answering chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## QA Chain\n",
    "def retriever_qa(file, query):\n",
    "    llm = get_llm()\n",
    "    retriever_obj = retriever(file)\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                    chain_type=\"stuff\", \n",
    "                                    retriever=retriever_obj, \n",
    "                                    return_source_documents=False)\n",
    "    response = qa.invoke(query)\n",
    "    return response['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recap how all the elements in our bot are linked. Note that RetrievalQA accepts an LLM (get_llm()) and a retriever object (an instance generated by retriever()) as arguments. However, the retriever is based on the vector store (vector_database()), which in turn needed an embeddings model (watsonx_embedding()) and chunks generated using a text splitter (text_splitter()). The text splitter, in turn, needed raw text, and this text was loaded from a PDF using PyPDFLoader. This effectively defines the core functionality of your QA bot!\n",
    "\n",
    "## Set up the Gradio interface\n",
    "Given that you have created the core functionality of the bot, the final item to define is the Gradio interface. Your Gradio interface should include:\n",
    "\n",
    "- A file upload functionality (provided by the File class in Gradio)\n",
    "- An input textbox where the question can be asked (provided by the Textbox class in Gradio)\n",
    "- An output textbox where the question can be answered (provided by the Textbox class in Gradio)\n",
    "Add the following code to add the Gradio interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gradio interface\n",
    "rag_application = gr.Interface(\n",
    "    fn=retriever_qa,\n",
    "    allow_flagging=\"never\",\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload PDF File\", file_count=\"single\", file_types=['.pdf'], type=\"filepath\"),  # Drag and drop file upload\n",
    "        gr.Textbox(label=\"Input Query\", lines=2, placeholder=\"Type your question here...\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Output\"),\n",
    "    title=\"RAG Chatbot\",\n",
    "    description=\"Upload a PDF document and ask any question. The chatbot will try to answer using the provided document.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add code to launch the application\n",
    "Finally, you need to add one more line to launch your application using port 7860:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/michael/repositories/cs_langchain/.venv/lib/python3.11/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/michael/repositories/cs_langchain/.venv/lib/python3.11/site-packages/gradio/route_utils.py\", line 321, in call_process_api\n",
      "    with utils.MatplotlibBackendMananger():\n",
      "  File \"/Users/michael/repositories/cs_langchain/.venv/lib/python3.11/site-packages/gradio/utils.py\", line 1026, in __enter__\n",
      "    import matplotlib\n",
      "  File \"/Users/michael/repositories/cs_langchain/.venv/lib/python3.11/site-packages/matplotlib/__init__.py\", line 1296, in <module>\n",
      "    rcParams['backend'] = os.environ.get('MPLBACKEND')\n",
      "    ~~~~~~~~^^^^^^^^^^^\n",
      "  File \"/Users/michael/repositories/cs_langchain/.venv/lib/python3.11/site-packages/matplotlib/__init__.py\", line 771, in __setitem__\n",
      "    raise ValueError(f\"Key {key}: {ve}\") from None\n",
      "ValueError: Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']\n"
     ]
    }
   ],
   "source": [
    "# Launch the app\n",
    "rag_application.launch(server_name=\"0.0.0.0\", server_port= 7860)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
